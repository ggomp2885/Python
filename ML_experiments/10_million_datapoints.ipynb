{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## currently unused functions for later reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Exporting to CSV\n",
    "%%time\n",
    "X.to_csv(f\"{nb_dir}/x_350m_combo_100f_rs1.csv\", index=False)\n",
    "# y.to_csv(f\"{nb_dir}/y_350m_combo_100f_rs1.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Iterative Imputer on untransformed dataset\n",
    "%%time\n",
    "imputer = IterativeImputer()\n",
    "imputer.fit(X_test)\n",
    "\n",
    "########\n",
    "# X_train_s_mm_i = imputer.fit_transform(X_train_s_scaled_mm)\n",
    "# X_train_s_mm_i = pd.DataFrame(X_train_s_mm_i)\n",
    "# display(X.tail(5))\n",
    "# print(f'{X_train_s_mm_i.isnull().sum()}')\n",
    "\n",
    "X_test_i = imputer.transform(X_test)\n",
    "X_test_i = pd.DataFrame(X_test_i)\n",
    "\n",
    "display(X_test_s_mm_i.tail(5))\n",
    "print(f'{X_test_s_mm_i.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_imports() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1633839602723
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def load_nb_imports_and_settings(run):\n",
    "    if run:\n",
    "        #python and datascience imports\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import os\n",
    "        import sys\n",
    "        import time\n",
    "        import itertools\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.datasets import make_classification\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer\n",
    "        from sagemaker import image_uris\n",
    "\n",
    "        # PIP installs\n",
    "        !{sys.executable} -m pip install --upgrade pip -qq\n",
    "        !{sys.executable} -m pip install sagemaker-experiments==0.1.24 -qq\n",
    "\n",
    "        # AWS S3 Imports\n",
    "        import boto3\n",
    "        import sagemaker\n",
    "        from sagemaker import get_execution_role\n",
    "        from sagemaker.session import Session\n",
    "        from sagemaker.inputs import TrainingInput\n",
    "\n",
    "        sess = boto3.Session()\n",
    "        sm = sess.client(\"sagemaker\")\n",
    "        role = get_execution_role()\n",
    "        region = boto3.Session().region_name\n",
    "        account_id = sess.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "\n",
    "        # AWS Experiment Imports\n",
    "        from smexperiments.tracker import Tracker\n",
    "        from smexperiments.experiment import Experiment\n",
    "        from smexperiments.trial import Trial\n",
    "        from smexperiments.trial_component import TrialComponent\n",
    "\n",
    "\n",
    "        # personal notebook settings\n",
    "        %config Completer.use_jedi = False\n",
    "        nb_dir = os.getcwd()\n",
    "\n",
    "        print('imports loading is complete and successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating mock classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def create_data(n_samples,  data_size_name, display_start, create_null, \n",
    "                drop_null, standardize, normalize, iterative_impute, \n",
    "                display_end, export_to_S3, export_to_FS):\n",
    "\n",
    "# making classification data\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=100, n_informative=80, \n",
    "                              n_redundant=10, n_repeated=5, n_classes=2, weights=[.7, .3], \n",
    "                              flip_y=.05, random_state=1)\n",
    "\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.DataFrame(y, columns=['target'])\n",
    "\n",
    "    \n",
    "# if display_start is true:\n",
    "    if display_start:\n",
    "        display(y.tail(2))\n",
    "        display(X.tail(2))\n",
    "    \n",
    "\n",
    "# creating null values:\n",
    "    if create_null:\n",
    "        X = X.mask(np.random.random(X.shape) < .001)\n",
    "        print(f'number of null values: {X.isnull().sum()}')\n",
    "\n",
    "              \n",
    "# dropping null values:\n",
    "    if drop_null:\n",
    "        X.dropna(inplace=True)\n",
    "        print(X.isnull().sum())\n",
    "        print(X.shape)\n",
    "\n",
    "              \n",
    "#splitting data into train / val sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, stratify=y, random_state=1)\n",
    "\n",
    "    df_train = pd.concat([y_train, X_train], axis=1)\n",
    "    df_val = pd.concat([y_test, X_test], axis=1)\n",
    "\n",
    "    display(df_train.tail(1), df_train.shape)\n",
    "    display(df_val.tail(1), df_val.shape)\n",
    "\n",
    "              \n",
    "# standardizing the dataset             \n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_s = pd.DataFrame(scaler.transform(X_train))\n",
    "        X_test_s = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "        display(X_test_s.tail(5))\n",
    "        display(X_train_s.tail(5))\n",
    "              \n",
    "              \n",
    "# normalize the dataset\n",
    "    if normalize:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        MMscaler = MinMaxScaler()\n",
    "        MMscaler.fit(X_train_s)\n",
    "\n",
    "        X_train_s_mm = pd.DataFrame(MMscaler.transform(X_train_s))\n",
    "        X_test_s_mm = pd.DataFrame(MMscaler.transform(X_test_s))\n",
    "\n",
    "        display(X_test_s_mm.tail(5))\n",
    "        display(X_train_s_mm.tail(5))\n",
    "              \n",
    "\n",
    "# iterative imputer on standardize and normalized dataset\n",
    "    if iterative_impute:\n",
    "        imputer = IterativeImputer()\n",
    "        imputer.fit(X_test_s_mm)\n",
    "\n",
    "        # X_train_s_mm_i = imputer.fit_transform(X_train_s_scaled_mm)\n",
    "        # X_train_s_mm_i = pd.DataFrame(X_train_s_mm_i)\n",
    "        # display(X.tail(5))\n",
    "        # print(f'{X_train_s_mm_i.isnull().sum()}')\n",
    "              \n",
    "        # X_test_s_mm_i = imputer.transform(X_test_s_mm)\n",
    "        X_test_s_mm_i = pd.DataFrame(X_test_s_mm_i)\n",
    "\n",
    "        \n",
    "# display_end\n",
    "    if display_end:\n",
    "        display(X_test_s_mm_i.tail(5))\n",
    "        print(f'{X_test_s_mm_i.isnull().sum()}')\n",
    "              \n",
    "              \n",
    "## Exporting to Parque, and Uploading to S3, and then removing from notebook storage    \n",
    "    if export_to_S3:\n",
    "        df_train.columns = df_train.columns.map(str)\n",
    "        df_val.columns = df_val.columns.map(str)\n",
    "\n",
    "        # creating directory to hold parquet files\n",
    "        try:\n",
    "            os.mkdir(f'{nb_dir}/{data_size_name}')\n",
    "            os.mkdir(f'{nb_dir}/{data_size_name}/train')\n",
    "            os.mkdir(f'{nb_dir}/{data_size_name}/val')\n",
    "        except:\n",
    "            print(\"Notebook Directories are already created\")\n",
    "\n",
    "        # to parquet\n",
    "        df_train.to_parquet(f\"{nb_dir}/{data_size_name}/train/train_{data_size_name}_100f_rs1.parquet\", index=False)\n",
    "        df_val.to_parquet(f\"{nb_dir}/{data_size_name}/val/val_{data_size_name}_100f_rs1.parquet\", index=False)\n",
    "\n",
    "        # creating S3 bucket (if not already created)\n",
    "        bucket = f\"sagemaker-experiments-10mill-{sess.region_name}-{account_id}\"\n",
    "        sess.client(\"s3\").create_bucket(Bucket=bucket)\n",
    "\n",
    "        # Uploading EFS folder \"3mill\" to S3 bucket\n",
    "        prefix = data_size_name\n",
    "        tracker_input_location = sagemaker.Session().upload_data(path=prefix, bucket=bucket, key_prefix=prefix)\n",
    "        print(f\"input spec: {tracker_input_location}\")\n",
    "\n",
    "        # deleting files in EFS\n",
    "        os.remove(f\"{nb_dir}/{data_size_name}/train/train_{data_size_name}_100f_rs1.parquet\")\n",
    "        os.remove(f\"{nb_dir}/{data_size_name}/val/val_{data_size_name}_100f_rs1.parquet\")\n",
    "\n",
    "        ## Specifying training and validation dataset location in S3\n",
    "        s3_train = TrainingInput(s3_data=f's3://{bucket}/{prefix}/train', content_type=\"application/x-parquet\")\n",
    "        s3_val = TrainingInput(s3_data=f's3://{bucket}/{prefix}/val', content_type=\"application/x-parquet\")\n",
    "        inputs = {'train': s3_train, 'validation': s3_val}\n",
    "              \n",
    "              \n",
    "# export to Feature Store\n",
    "    if export_to_FS:\n",
    "        pass\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Track custom metrics in experiment metadata by creating a Tracker object\n",
    "def create_experiment(experiment_name, hyperparam_options, norm_mean, norm_std):\n",
    "    \n",
    "    norm_mean = 0.1307\n",
    "    norm_std = 0.3081\n",
    "\n",
    "    with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "        tracker.log_parameters({\"normalization_mean\": normalization_mean, \"normalization_std\": normalization_std})\n",
    "        tracker.log_input(name=\"10mill-dataset\", media_type=\"s3/uri\", value = tracker_input_location)\n",
    "    preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "\n",
    "    ## Creating Experiment\n",
    "    try:\n",
    "        exp_10m_datapoints = Experiment.create(experiment_name=\"10m-datapoints-6\", \n",
    "                                               description=\"Trials of experiments using 10 million datapoints or more\", \n",
    "                                               sagemaker_boto_client=sm)\n",
    "    except:\n",
    "        print(\"experiment already created\")\n",
    "\n",
    "\n",
    "    ## Setting up Hyperparameters list\n",
    "    hyperparam_options = {\"eta\": [0.1, 0.5], \"num_round\": [10, 20]}\n",
    "    hypnames, hypvalues = zip(*hyperparam_options.items())\n",
    "    trial_hyperparameter_set = [dict(zip(hypnames, h)) for h in itertools.product(*hypvalues)]\n",
    "    trial_hyperparameter_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TrainingLoop to create multiple Trials and Trial components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Both instance_count and instance_type are required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3eebf1f337df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                     \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                     \u001b[0mhyperparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                     enable_sagemaker_metrics=True)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Launch a training job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_uri, role, instance_count, instance_type, volume_size, volume_kms_key, max_run, input_mode, output_path, output_kms_key, base_job_name, sagemaker_session, hyperparameters, tags, subnets, security_group_ids, model_uri, model_channel_name, metric_definitions, encrypt_inter_container_traffic, use_spot_instances, max_wait, checkpoint_s3_uri, checkpoint_local_path, enable_network_isolation, rules, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_config, disable_profiler, environment, max_retry_attempts, **kwargs)\u001b[0m\n\u001b[1;32m   1910\u001b[0m             \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m             \u001b[0mmax_retry_attempts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_retry_attempts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1912\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1913\u001b[0m         )\n\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, role, instance_count, instance_type, volume_size, volume_kms_key, max_run, input_mode, output_path, output_kms_key, base_job_name, sagemaker_session, tags, subnets, security_group_ids, model_uri, model_channel_name, metric_definitions, encrypt_inter_container_traffic, use_spot_instances, max_wait, checkpoint_s3_uri, checkpoint_local_path, rules, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, enable_network_isolation, profiler_config, disable_profiler, environment, max_retry_attempts, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minstance_count\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minstance_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both instance_count and instance_type are required.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Both instance_count and instance_type are required."
     ]
    }
   ],
   "source": [
    "# downloading xgboost container from aws api\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.3-1\", image_scope='training')\n",
    "\n",
    "for i in trial_hyperparameter_set:\n",
    "    hyp_append = \"-\".join([str(elm).replace(\".\", \"-\") for elm in i.values()])\n",
    "    trial_name = f\"Trial-xgboost-credit-risk-training-{hyp_append}-{int(time.time())}\"\n",
    "    xgboost_credit_risk_trial = Trial.create(trial_name = trial_name,\n",
    "                                             experiment_name=exp_10m_datapoints.experiment_name,\n",
    "                                             sagemaker_boto_client=sm)\n",
    "    xgboost_credit_risk_trial.add_trial_component(preprocessing_trial_component)\n",
    "    \n",
    "    xgboost_credit_risk_estimator = sagemaker.estimator.Estimator(\n",
    "                                    xgboost_container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.c4.xlarge',\n",
    "                                    output_path=f's3://{bucket}/{prefix}/output',\n",
    "                                    sagemaker_session=sagemaker.Session(),\n",
    "                                    hyperparameters=i,\n",
    "                                    enable_sagemaker_metrics=True)\n",
    "\n",
    "    # Launch a training job\n",
    "    xgboost_credit_risk_estimator.fit(inputs, \n",
    "                                      job_name=trial_name, \n",
    "                                      experiment_config={\"ExperimentName\": exp_10m_datapoints.experiment_name,\n",
    "                                                         \"TrialName\": xgboost_credit_risk_trial.trial_name,\n",
    "                                                         \"TrialComponentDisplayName\": f\"TC-xgboost-credit-risk-training-{hyp_append}-{int(time.time())}\"})\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to create new datasets and new experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>56.697762</td>\n",
       "      <td>10.389035</td>\n",
       "      <td>-6.750176</td>\n",
       "      <td>-7.334447</td>\n",
       "      <td>2.450279</td>\n",
       "      <td>1.812355</td>\n",
       "      <td>0.582958</td>\n",
       "      <td>-4.031362</td>\n",
       "      <td>7.411121</td>\n",
       "      <td>1.875239</td>\n",
       "      <td>...</td>\n",
       "      <td>3.300077</td>\n",
       "      <td>-0.836226</td>\n",
       "      <td>-9.014265</td>\n",
       "      <td>6.019924</td>\n",
       "      <td>4.393018</td>\n",
       "      <td>-9.695271</td>\n",
       "      <td>9.451914</td>\n",
       "      <td>0.604435</td>\n",
       "      <td>-9.321836</td>\n",
       "      <td>-10.317076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-11.427159</td>\n",
       "      <td>16.045496</td>\n",
       "      <td>-3.136223</td>\n",
       "      <td>-8.693787</td>\n",
       "      <td>1.001132</td>\n",
       "      <td>-8.037895</td>\n",
       "      <td>5.733319</td>\n",
       "      <td>1.931590</td>\n",
       "      <td>2.472942</td>\n",
       "      <td>2.805699</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.262238</td>\n",
       "      <td>-8.303221</td>\n",
       "      <td>0.228275</td>\n",
       "      <td>1.646426</td>\n",
       "      <td>5.146915</td>\n",
       "      <td>-0.822909</td>\n",
       "      <td>2.227395</td>\n",
       "      <td>1.856326</td>\n",
       "      <td>0.640513</td>\n",
       "      <td>-2.101273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-19.067626</td>\n",
       "      <td>-5.601985</td>\n",
       "      <td>-2.143455</td>\n",
       "      <td>4.383317</td>\n",
       "      <td>2.256321</td>\n",
       "      <td>-6.871535</td>\n",
       "      <td>-2.631641</td>\n",
       "      <td>-1.023628</td>\n",
       "      <td>3.798189</td>\n",
       "      <td>1.870741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806360</td>\n",
       "      <td>-2.978635</td>\n",
       "      <td>0.956495</td>\n",
       "      <td>-0.850825</td>\n",
       "      <td>0.983967</td>\n",
       "      <td>6.699350</td>\n",
       "      <td>2.217536</td>\n",
       "      <td>0.107022</td>\n",
       "      <td>-6.972750</td>\n",
       "      <td>2.460253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>11.439388</td>\n",
       "      <td>10.300106</td>\n",
       "      <td>1.946154</td>\n",
       "      <td>-1.111633</td>\n",
       "      <td>-3.816722</td>\n",
       "      <td>-8.985397</td>\n",
       "      <td>6.554141</td>\n",
       "      <td>14.503746</td>\n",
       "      <td>-2.183606</td>\n",
       "      <td>-2.718096</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.325456</td>\n",
       "      <td>0.820471</td>\n",
       "      <td>-10.949983</td>\n",
       "      <td>-6.653628</td>\n",
       "      <td>2.884723</td>\n",
       "      <td>6.614931</td>\n",
       "      <td>2.013875</td>\n",
       "      <td>-0.317921</td>\n",
       "      <td>-5.039085</td>\n",
       "      <td>-4.301446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-11.120939</td>\n",
       "      <td>-4.895588</td>\n",
       "      <td>-6.142724</td>\n",
       "      <td>1.498014</td>\n",
       "      <td>-3.597584</td>\n",
       "      <td>-4.795121</td>\n",
       "      <td>1.071561</td>\n",
       "      <td>5.687048</td>\n",
       "      <td>4.248828</td>\n",
       "      <td>-3.399793</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.892585</td>\n",
       "      <td>2.960362</td>\n",
       "      <td>-6.571430</td>\n",
       "      <td>0.493254</td>\n",
       "      <td>-3.314841</td>\n",
       "      <td>0.579006</td>\n",
       "      <td>0.497357</td>\n",
       "      <td>0.088681</td>\n",
       "      <td>-8.018253</td>\n",
       "      <td>-3.459935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1         2         3         4         5         6   \\\n",
       "95  56.697762  10.389035 -6.750176 -7.334447  2.450279  1.812355  0.582958   \n",
       "96 -11.427159  16.045496 -3.136223 -8.693787  1.001132 -8.037895  5.733319   \n",
       "97 -19.067626  -5.601985 -2.143455  4.383317  2.256321 -6.871535 -2.631641   \n",
       "98  11.439388  10.300106  1.946154 -1.111633 -3.816722 -8.985397  6.554141   \n",
       "99 -11.120939  -4.895588 -6.142724  1.498014 -3.597584 -4.795121  1.071561   \n",
       "\n",
       "           7         8         9   ...        90        91         92  \\\n",
       "95  -4.031362  7.411121  1.875239  ...  3.300077 -0.836226  -9.014265   \n",
       "96   1.931590  2.472942  2.805699  ... -1.262238 -8.303221   0.228275   \n",
       "97  -1.023628  3.798189  1.870741  ...  0.806360 -2.978635   0.956495   \n",
       "98  14.503746 -2.183606 -2.718096  ... -2.325456  0.820471 -10.949983   \n",
       "99   5.687048  4.248828 -3.399793  ... -4.892585  2.960362  -6.571430   \n",
       "\n",
       "          93        94        95        96        97        98         99  \n",
       "95  6.019924  4.393018 -9.695271  9.451914  0.604435 -9.321836 -10.317076  \n",
       "96  1.646426  5.146915 -0.822909  2.227395  1.856326  0.640513  -2.101273  \n",
       "97 -0.850825  0.983967  6.699350  2.217536  0.107022 -6.972750   2.460253  \n",
       "98 -6.653628  2.884723  6.614931  2.013875 -0.317921 -5.039085  -4.301446  \n",
       "99  0.493254 -3.314841  0.579006  0.497357  0.088681 -8.018253  -3.459935  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports loading is complete and successful\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_classification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-245054fa4b4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mdisplay_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mexport_to_S3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             export_to_FS = False)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mcreate_data\u001b[0;34m(n_samples, data_size_name, display_start, create_null, drop_null, standardize, normalize, iterative_impute, display_end, export_to_S3, export_to_FS)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_classification' is not defined"
     ]
    }
   ],
   "source": [
    "load_nb_imports_and_settings(run=True)\n",
    "\n",
    "create_data(n_samples= 1_000, \n",
    "            data_size_name=\"100k\",\n",
    "            display_start = True,\n",
    "            create_null = False,\n",
    "            drop_null = False,\n",
    "            standardize = False,\n",
    "            normalize = False,\n",
    "            iterative_impute = False,\n",
    "            display_end = False,\n",
    "            export_to_S3 = False,\n",
    "            export_to_FS = False)\n",
    "\n",
    "\n",
    "# create_experiment(experiment_name=\"new_1\", \n",
    "#                   hyperparam_options= {\"eta\": [0.1, 0.5], \"num_round\": [10, 20]}, \n",
    "#                   norm_mean=norm_mean, \n",
    "#                   norm_std=norm_std)\n",
    "\n",
    "# training_loop(trial_hyperparameter_set = trial_hyperparameter_set,\n",
    "#               instance_count=2, \n",
    "#               instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training loop ---- specific training instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alternative training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f40bd3981dbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# create trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrial_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"cvs-training-job-{num_cv}-cv-{int(time.time())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     cvs_trial = Trial.create(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "for i, num_cv in enumerate([2, 3, 4, 5, 6]):\n",
    "    cvs_trial = Trial.create(trial_name=f\"cvs-training-job-{num_cv}-cv-{int(time.time())}\",\n",
    "                             experiment_name=experiment_10m_datapoints.experiment_name,\n",
    "                             sagemaker_boto_client=sm,)\n",
    "    cvs_trial.add_trial_component(preprocessing_trial_component)\n",
    "\n",
    "    \n",
    "    # all input configurations, parameters, and metrics in estimator are automatically tracked\n",
    "    estimator = GradientBoostingClassifier(\n",
    "                    role=role,\n",
    "                    sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n",
    "                    instance_count=1,\n",
    "                    instance_type=\"ml.c4.xlarge\",\n",
    "                    enable_sagemaker_metrics=True,\n",
    "                    hyperparameters = num_cv)\n",
    "    \n",
    "    \n",
    "    # Now associate the estimator with the Experiment and Trial\n",
    "    estimator.fit(inputs = {\"training\": inputs},\n",
    "                            job_name = \"cnn-training-job-{}\".format(int(time.time())),\n",
    "                            wait=True,\n",
    "                            experiment_config = {\"TrialName\": cnn_trial.trial_name,\n",
    "                                                 \"TrialComponentDisplayName\": \"Training\",})\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Time test for running c_v_s() on GBC with untransformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1633572794372
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692\n",
      "CPU times: user 1min 34s, sys: 44.4 ms, total: 1min 34s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "scoring = \"f1\"\n",
    "\n",
    "cv_results = cross_val_score(GradientBoostingClassifier(), X_03, y_03, cv=2, scoring=scoring)\n",
    "print(round(cv_results.mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Testing running time for GridsearchCV() on GBC with untransformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1633816980651
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ccp_alpha', 'criterion', 'init', 'learning_rate', 'loss', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_iter_no_change', 'presort', 'random_state', 'subsample', 'tol', 'validation_fraction', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gbc.get_params().keys()\n",
    "GradientBoostingClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1633825045457
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3402/618504648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp_X_03\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_03\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp_y_03\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_03\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_X_03\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "np_X_03 = X_03.values\n",
    "np_y_03 = y_03.values\n",
    "display(np_X_03.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest mean cv score: 0.9021680962218668, with params: {'max_depth': 7, 'n_estimators': 600}\n",
      "CPU times: user 1h 34min 35s, sys: 356 ms, total: 1h 34min 35s\n",
      "Wall time: 1h 34min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model_gbc = GridSearchCV(estimator = GradientBoostingClassifier(), \n",
    "                         param_grid = {'max_depth': [4,6,7],\n",
    "                                       'n_estimators': [300,600]},\n",
    "                         cv=2,\n",
    "                         scoring = scoring)\n",
    "\n",
    "model_gbc.fit(np_X_03, np_y_03)\n",
    "print(f'highest mean cv score: {model_gbc.best_score_}, with params: {model_gbc.best_params_}')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
